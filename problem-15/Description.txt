🔁 What is an LRU Cache?
LRU (Least Recently Used) Cache keeps track of the most recently used items. If the cache is full and a new item needs to be inserted, it removes the least recently used item first.
To implement an LRU (Least Recently Used) Cache, we need to support the following operations in O(1) time:
    • get(key): Return the value if the key exists in the cache, else return -1.
    • put(key, value): Insert the key-value pair into the cache. If the key already exists, update the value. If the cache exceeds capacity, remove the least recently used item.
🔧 Key Components:
    • A hashmap to store key → node mappings.
    • A doubly linked list to maintain usage order:
        ◦ Most recently used item is at the head.
        ◦ Least recently used item is at the tail.
✅ Code Explanation (Line by Line)

1. Node class — represents each entry in the cache
class Node:
    def __init__(self, key, value):
        self.key = key          # The unique identifier of the item
        self.value = value      # The data value for the key
        self.prev = self.next = None  # Pointers for the doubly linked list

You need a Node to store both the key and value. Each node also has:
    • prev: points to the previous node
    • next: points to the next node
➡️ This structure is for doubly linked list so you can insert and remove nodes in O(1) time.

2. LRUCache class — main cache system
class LRUCache:
    def __init__(self, capacity: int):
        self.capacity = capacity       # Maximum size of the cache
        self.cache = {}                # Dictionary to map keys to nodes

        # Dummy nodes to mark the boundaries
        self.head = Node(0, 0)         # Head = Most recently used
        self.tail = Node(0, 0)         # Tail = Least recently used

        self.head.next = self.tail     # Initially head and tail are connected
        self.tail.prev = self.head

    • capacity: maximum number of items allowed in the cache.
    • cache: a dictionary (key → node) for fast access.
    • head and tail: dummy nodes to simplify insertion/removal.
    • Nodes closer to head are more recently used.
    • Nodes closer to tail are less recently used.
      
3. Remove a node from the list
def _remove(self, node: Node):
    """Remove node from the linked list."""
    prev = node.prev
    nxt = node.next
    prev.next = nxt
    nxt.prev = prev
    • Disconnects node from the list.
    • Changes the pointers of node.prev and node.next to skip over the node.
    • This operation is O(1).
      
4. Insert a node right after head (most recently used)
def _insert_at_front(self, node: Node):
    """Insert node right after head (most recently used)."""
    node.prev = self.head
    node.next = self.head.next
    self.head.next.prev = node
    self.head.next = node
    • Inserts the node just after the head, meaning it’s now the most recently used.
    • Again, this is done in O(1) time.
    • We adjust 4 pointers:
    • node's prev and next
    • The old first node’s prev
    • The head's next
      


5. Get value by key
def get(self, key: int) -> int:
    if key in self.cache:
        node = self.cache[key]
        # Move to front
        self._remove(node)
        self._insert_at_front(node)
        return node.value
    return -1
    • Checks if the key exists in the cache.
    • If found:
    • Retrieves the node
    • Moves it to the front (most recently used)
    • Returns its value
    • If not found:
    • Returns -1
      
6. Put a key-value pair into the cache
def put(self, key: int, value: int):
    if key in self.cache:
        self._remove(self.cache[key])  # Remove old node if already exists

    node = Node(key, value)           # Create a new node
    self.cache[key] = node            # Store in the hashmap
    self._insert_at_front(node)       # Make it most recently used

    if len(self.cache) > self.capacity:
        # Remove LRU from the tail
        lru = self.tail.prev          # Node just before tail is LRU
        self._remove(lru)             # Remove from linked list
        del self.cache[lru.key]       # Remove from hashmap
If the key already exists:

    • Remove its current node (because we’ll insert the new one).
    • Add the new key-value node.
    • Insert it at the front (most recently used).
    • If cache size exceeds capacity:
    • Remove the least recently used item:
        ◦ It is the node just before the dummy tail.
        ◦ Remove it from both the list and the dictionary.

🔢 Test Code:

cache = LRUCache(2)
cache.put(1, 1)
cache.put(2, 2)
print(cache.get(1))    # Output: ?
cache.put(3, 3)
print(cache.get(2))    # Output: ?
cache.put(4, 4)
print(cache.get(1))    # Output: ?
print(cache.get(3))    # Output: ?
print(cache.get(4))    # Output: ?
🔍 Line-by-Line Output Meaning:
1️⃣ cache.get(1)
    • 🔎 You added keys 1 and 2 → Cache is {1=1, 2=2}
    • You call get(1), which returns 1 (the value stored with key 1).
    • It also marks key 1 as most recently used.
✅ Output: 1
2️⃣ cache.put(3, 3)
    • Cache is full (capacity = 2).
    • Adding key 3 requires removing the least recently used (LRU) item.
    • Since key 1 was just used, key 2 is least recently used, so it gets removed.
    • Cache becomes {1=1, 3=3}
3️⃣ cache.get(2)
    • Key 2 was evicted in the last step.
    • It is no longer in the cache.
❌ Output: -1 (not found)
4️⃣ cache.put(4, 4)
    • Cache is full again → must evict LRU
    • Current cache: {1=1, 3=3}
    • Key 3 was just added, so key 1 is now the LRU
    • Evict key 1, insert key 4
    • Cache becomes {3=3, 4=4}
5️⃣ cache.get(1)
    • Key 1 was evicted in the previous step
❌ Output: -1 (not found)
6️⃣ cache.get(3)
    • Key 3 is still in cache
✅ Output: 3
7️⃣ cache.get(4)
    • Key 4 is in cache
✅ Output: 4
📤 Final Output:
1
-1
-1
3
4
🧠 Summary
Operation
Description
Output
get(1)
Found, moved to front
1
get(2)
Not found (evicted when 3 was added)
-1
get(1) again
Not found (evicted when 4 was added)
-1
get(3)
Found
3
get(4)
Found
4


